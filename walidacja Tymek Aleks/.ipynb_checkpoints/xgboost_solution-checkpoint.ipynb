{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577ffc6b",
   "metadata": {},
   "source": [
    "# Credit score classification project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679e489",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "First of all we need to split our data into train, validation and test sets.\n",
    "\n",
    "Even though there is a test set given in the competition files, we don't have any access to the target variables, so we decided to use train.csv file only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv') # dataframe contains everything, not only train\n",
    "# test = pd.read_csv('test.csv') this one doesn't works - missing target value\n",
    "\n",
    "test_size = 0.2\n",
    "X = data.drop(columns=[\"Credit_Score\"]).copy()\n",
    "y = data[\"Credit_Score\"] # the target \n",
    "\n",
    "\n",
    "X_rem, X_test, y_rem, y_test = train_test_split(X,y, test_size=test_size, shuffle=False) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15202eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged in order to perfrom preprocessing efficiently\n",
    "train = X_train.join(y_train)\n",
    "val = X_val.join(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc4e17",
   "metadata": {},
   "source": [
    "# Data Examination\n",
    "\n",
    "Let's have a look into our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fa99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Customer_ID\"].value_counts().loc[train[\"Customer_ID\"].value_counts() != 8].size \n",
    "# every customer shows up exactly 8 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Credit_Score'].isna().sum() # all target values are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39344cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train[\"Credit_Score\"]) # target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"Name\", \"SSN\", \"Customer_ID\"]].head(10) # name, customer id and SSN are redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa224b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_alter = ['Age', 'Annual_Income', 'Num_of_Loan','Num_of_Delayed_Payment',\n",
    "                    'Changed_Credit_Limit', 'Outstanding_Debt',\n",
    "                    'Amount_invested_monthly', 'Monthly_Balance']\n",
    "\n",
    "train[columns_to_alter].iloc[10:20] # some columns have weird signs - \"_\" and are of wrong format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8878bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how different are categorical columns\n",
    "\n",
    "print(train['Payment_of_Min_Amount'].unique(), \"\\n\\n\",\n",
    "      train['Occupation'].unique(), \"\\n\\n\",\n",
    "      train[\"Credit_Mix\"].unique(), \"\\n\\n\",\n",
    "      train[\"Payment_Behaviour\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account age, and behaviour are in unfriendly format\n",
    "\n",
    "train[[\"Credit_History_Age\", \"Payment_Behaviour\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12019786",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.corr(method=\"spearman\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2c533",
   "metadata": {},
   "source": [
    "## Search for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = train.describe()\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029899c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()\n",
    "for col in train.columns:\n",
    "    df[col] = pd.to_numeric(train[col], errors='coerce') # errors='coerce', then invalid parsing will be set as NaN\n",
    "\n",
    "# distribution of all the variables\n",
    "df.hist(figsize=(30, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6121d0",
   "metadata": {},
   "source": [
    "Some of the plots look very unusual - they consist of only one column. \n",
    "It probably means that those features contain some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose outliers by looking at the plots\n",
    "\n",
    "columns_with_outliers = [\"Age\", \"Annual_Income\", \"Num_Bank_Accounts\", \n",
    "                         \"Num_Credit_Card\", \"Interest_Rate\", \n",
    "                         \"Num_of_Loan\", \"Num_of_Delayed_Payment\",\n",
    "                         \"Num_Credit_Inquiries\", \"Total_EMI_per_month\"]\n",
    "\n",
    "\n",
    "df[columns_with_outliers].quantile(0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d69d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between columns \n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2facd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly_Inhand_Salary is strongly correlated with Monthly_Balance and \n",
    "# Amount_Invested_Monthly but those are not correlated with each other\n",
    "\n",
    "plt.bar(x = train[\"Credit_Score\"].unique(),\n",
    "        height=train.loc[df[\"Num_of_Loan\"]>=15][\"Credit_Score\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c77f97e",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e22ac8",
   "metadata": {},
   "source": [
    "First of all we git rid of columns that carry the same information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6422de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_columns(df):\n",
    "    df = df.drop([\"Name\", \"SSN\", \"ID\"], axis=\"columns\") # no added value, all the information in Customer_ID\n",
    "    df[\"Customer_ID\"] = df[\"Customer_ID\"].apply(lambda x: int(x[4:], 16)) # convert to int\n",
    "    \n",
    "    print(\"Columns containing id-like information preprocessed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b15bc0",
   "metadata": {},
   "source": [
    "Here we literally 'clean' the data, removing unnecessary signs that appear randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fd3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_numeric_data(col):\n",
    "    # some of the rows contain \"_\" sign\n",
    "    col.astype(str).replace(\"_\", \"\")\n",
    "    return pd.to_numeric(col, errors=\"coerce\") # errors='coerce', then invalid parsing will be set as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a2cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cleaning numeric data where necessary\n",
    "def altering(df):\n",
    "    \n",
    "    columns_to_alter = ['Age', 'Annual_Income', 'Num_of_Loan','Num_of_Delayed_Payment',\n",
    "                        'Changed_Credit_Limit', 'Outstanding_Debt',\n",
    "                        'Amount_invested_monthly', 'Monthly_Balance']\n",
    "\n",
    "    df[columns_to_alter] = df[columns_to_alter].apply(clear_numeric_data, axis=1)\n",
    "\n",
    "    print(\"Numeric data preprocessed. Columns with numeric values contain numeric only variables. Changed columns types\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dce4ef",
   "metadata": {},
   "source": [
    "Function below helps with removal outlying values from a column.\n",
    "If parameter (use_quantiles) is False, removes outliers outside given set range (a, b).\n",
    "Otherwise removes top 2 quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c84e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_outliers(column, a=0, b=0.98, use_quantiles=True):\n",
    "    # function helps removing quantiles from a column, \n",
    "    # if parameter (use_quantiles) is False, removes outliers outside given range (a, b)\n",
    "    # otherwise removes top quantiles\n",
    "    \n",
    "    col = np.where(a < column, column, float('nan'))\n",
    "    if use_quantiles:\n",
    "        return np.where(col <= column.quantile(b), column, float('nan'))\n",
    "    return np.where(col <= b, column, float('nan'))\n",
    "\n",
    "\n",
    "def handle_outliers(df):\n",
    "    df[\"Age\"] = np.where((0 > df[\"Age\"]), -df[\"Age\"], df[\"Age\"]) # deleting weird outliers\n",
    "    df['Age'] = delete_outliers(df[\"Age\"], 0, 100, use_quantiles=False)\n",
    "    \n",
    "    df[\"Annual_Income\"] = delete_outliers(df[\"Annual_Income\"])\n",
    "    df[\"Num_Bank_Accounts\"] = delete_outliers(df[\"Num_Bank_Accounts\"])\n",
    "    df[\"Num_of_Loan\"] = delete_outliers(df[\"Num_of_Loan\"])\n",
    "    df[\"Interest_Rate\"] = delete_outliers(df[\"Interest_Rate\"])\n",
    "    df[\"Num_Credit_Card\"] = delete_outliers(df[\"Num_Credit_Card\"], 0, 0.97)\n",
    "    df[\"Num_of_Delayed_Payment\"] = delete_outliers(df[\"Num_of_Delayed_Payment\"])\n",
    "    df[\"Num_Credit_Inquiries\"] = delete_outliers(df[\"Num_Credit_Inquiries\"])\n",
    "    df[\"Total_EMI_per_month\"] = delete_outliers(df[\"Total_EMI_per_month\"], 0, 0.95)\n",
    "\n",
    "\n",
    "    print(\"Deleted outliers\")   \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfa837",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f55582",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Occupation\"] = np.where(train[\"Occupation\"] == \"_______\", \"Unknown\", train[\"Occupation\"])\n",
    "occupations_list = train[\"Occupation\"].unique()\n",
    "occupations_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641a778",
   "metadata": {},
   "source": [
    "Sometimes when data is missing, it's possible to avail rows that pertain to the same customer and contain missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7200d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we fill the information for customers that have other rows with full information available\n",
    "train['Type_of_Loan'].fillna(train.groupby('Customer_ID')['Type_of_Loan'].first(), inplace=True)\n",
    "# later we fill with ''\n",
    "train['Type_of_Loan'].fillna('', inplace=True)\n",
    "    \n",
    "loan_types_list =  train['Type_of_Loan'].value_counts().head(9).index[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b585c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(df):\n",
    "    # encode columns where necessary (Credit_Mix, Payment_of_Min_Amount, Credit_Score)\n",
    "    # mapping\n",
    "    credit_mix_dict = {'Bad':0, 'Standard':1, 'Good':2, '_':float('nan')}\n",
    "    poma_dict = {'No':0, 'Yes':1, 'NM':float('nan')}\n",
    "    credit_score_dict = {'Poor':0, 'Standard':1, 'Good':2}\n",
    "    month_dict = {\"January\": 1, \"February\": 2, \"March\": 3, \"April\": 4, \"May\": 5,\n",
    "                \"June\": 6, \"July\": 7, \"August\": 8}\n",
    "    \n",
    "    df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].map(poma_dict)\n",
    "    df['Credit_Mix'] = df['Credit_Mix'].map(credit_mix_dict)\n",
    "    df['Credit_Score'] = df['Credit_Score'].map(credit_score_dict)\n",
    "    df[\"Month\"] = df[\"Month\"].map(month_dict)\n",
    "    \n",
    "    # Payment_Behaviour column brings two informations, one about spending, other about value payments\n",
    "    df[\"Payment_Behaviour\"] = df[\"Payment_Behaviour\"].replace('!@9#%8', 'Unknown_spent_Unknown_value_payments')\n",
    "    split_payment = lambda x:  ([x.split(\"_\")[0], x.split(\"_\")[2]] if (x is not None) else [\"Unknown\", \"Unknown\"])\n",
    "    df[\"Payment_Behaviour\"] = df[\"Payment_Behaviour\"].apply(split_payment)\n",
    "    df[[\"Spending_Behaviour\", \"Value_Payments\"]] = pd.DataFrame(df[\"Payment_Behaviour\"].tolist(), index=df.index)\n",
    "    \n",
    "    spending_dict = {'Low':0, 'High':1, 'Unknown':float('nan')}\n",
    "    value_dict = {'Small':0, 'Medium':1, 'Large':2,  'Unknown':float('nan')}\n",
    "    df['Spending_Behaviour'] = df['Spending_Behaviour'].map(spending_dict)\n",
    "    df['Value_Payments'] = df['Value_Payments'].map(value_dict)\n",
    "    \n",
    "    del df[\"Payment_Behaviour\"]\n",
    "    \n",
    "    \n",
    "    for loan_type in loan_types_list: # the single types of loans\n",
    "        df[loan_type] = df['Type_of_Loan'].str.contains(loan_type).astype(\"bool\")\n",
    "    del df[\"Type_of_Loan\"]\n",
    "    \n",
    "    \n",
    "    # credit history age    \n",
    "    df['Credit_History_Age'] = df['Credit_History_Age'].apply(history_age)\n",
    "    \n",
    "    # Occupation - ____ for uneployed\n",
    "    df[\"Occupation\"] = np.where(df[\"Occupation\"] == \"_______\", \"Unknown\", df[\"Occupation\"])\n",
    "    df[\"Occupation\"] = np.where(df[\"Occupation\"].isin(occupations_list), df[\"Occupation\"], \"Unknown\") # to handle different occupations in test data\n",
    "    df = df.join(pd.get_dummies(df['Occupation']))\n",
    "    del df[\"Occupation\"]\n",
    "\n",
    "    print(\"Categorical columns with string values encoded. Added new columns where necessary (one-hot encoding)\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1993b7",
   "metadata": {},
   "source": [
    "## handling NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268925b9",
   "metadata": {},
   "source": [
    "We will impute missing data. In case of some values we look up to rows containing\n",
    "data about the same customer, using mode/median of values pertaining to him. If for customer\n",
    "there are no other rows, we impute using globally most common values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_values(df): # this works but very slowly\n",
    "    \n",
    "    continuous_values = ['Monthly_Inhand_Salary', 'Amount_invested_monthly',\n",
    "                         'Monthly_Balance', 'Credit_History_Age', \"Outstanding_Debt\",\n",
    "                         \"Changed_Credit_Limit\", \"Annual_Income\"]\n",
    "    \n",
    "    for column in continuous_values:\n",
    "        df[column] = df[column].fillna(df.groupby('Customer_ID')[column].transform(\"mean\"))\n",
    "    # using mean is not causing any trouble here, we're practically taking the value that appears\n",
    "    # in the rows with the same id\n",
    "        \n",
    "    \n",
    "    discrete_columns = [\"Age\", \"Num_Credit_Inquiries\", \"Num_of_Loan\", \"Credit_Mix\",\n",
    "                        \"Num_of_Delayed_Payment\", \"Num_Credit_Inquiries\", \n",
    "                        \"Spending_Behaviour\", \"Payment_of_Min_Amount\", \"Value_Payments\"]\n",
    "    # for discrete values we'll impute nans with mode\n",
    "    for column in discrete_columns:\n",
    "        #train[column].fillna(train.groupby('Customer_ID')[column].agg(lambda x: pd.Series.mode(x)[0]), inplace=True) \n",
    "        df[column].fillna(df.groupby('Customer_ID')[column].transform('median'), inplace=True) \n",
    "        pass\n",
    "    \n",
    "    df = df.fillna(df.median()) # in case a customer doesn't have any entries\n",
    "\n",
    "\n",
    "    print(\"NA values imputed within numeric columns\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing Credit_History_Age so that's continuous\n",
    "import re\n",
    "def history_age(age):\n",
    "    try : \n",
    "        years = int(re.findall('[0-9]+', age)[0])\n",
    "        month = int(re.findall('[0-9]+', age)[1])\n",
    "        return years*12 + month\n",
    "    except :\n",
    "        return np.nan\n",
    "    \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc32c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_discrete_string(df):\n",
    "\n",
    "    discrete_string_columns = [\"Occupation\", \"Type_of_Loan\", \"Payment_Behaviour\"]\n",
    "    # fill missing data in columns that are strings\n",
    "\n",
    "    for column in discrete_string_columns:\n",
    "        most_common_globally = df[column].agg(lambda x: pd.Series.mode(x)[0])\n",
    "        df[column].fillna(df.groupby('Customer_ID')[column].agg(\n",
    "            lambda x: (most_common_globally if len(pd.Series.mode(x)) == 0 else pd.Series.mode(x)[0])\n",
    "            ), inplace=True) \n",
    "        \n",
    "\n",
    "    print(\"NA values imputed within text columns\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225352d5",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e846699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "drop_transformer = FunctionTransformer(id_columns)\n",
    "altering_transformer = FunctionTransformer(altering)\n",
    "outlier_transformer = FunctionTransformer(handle_outliers)\n",
    "categorical_transformer = FunctionTransformer(encode_categorical)\n",
    "imputer_transformer = FunctionTransformer(impute_values)\n",
    "discrete_imputer_transformer = FunctionTransformer(impute_discrete_string)\n",
    "\n",
    "# all the afore declared processings applied\n",
    "prepipe = Pipeline([\n",
    "    (\"drop\", drop_transformer),\n",
    "    (\"altering\", altering_transformer),\n",
    "    (\"outliers\", outlier_transformer),\n",
    "    (\"impute_discrete_string\", discrete_imputer_transformer),\n",
    "    (\"categorical\", categorical_transformer),\n",
    "    (\"impute\", imputer_transformer)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = prepipe.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outliers - we change the outliers pipe\n",
    "described = train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a53f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hist(figsize=(30, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ea1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.corr(), cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb2f07",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f917687",
   "metadata": {},
   "source": [
    "We will initially create the models, to check which features have the highest importance.\n",
    "Our intention is to drop the irrelevant later after the 1st model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd12554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fee98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "y = train['Credit_Score']\n",
    "del train['Credit_Score']\n",
    "y_train = y\n",
    "X_train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "val = prepipe.transform(val)\n",
    "X_val = val\n",
    "y_val = val[\"Credit_Score\"]\n",
    "del X_val[\"Credit_Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef049a4",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f879fc81",
   "metadata": {},
   "source": [
    "1st fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174eb5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce53215",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=120)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c955f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55f76d",
   "metadata": {},
   "source": [
    "# Verify feature importance in random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07266953",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf_model.feature_importances_,\n",
    "                                   X_val.columns,\n",
    "                                   columns=['Importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.sort_values(by='Importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f794a3c",
   "metadata": {},
   "source": [
    "# Dropping highly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fa326",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_delete = []\n",
    "for i in range(len(X_train.columns)):\n",
    "    for j in range(i+1, len(X_train.columns)):\n",
    "        # we iterate over every pair of columns\n",
    "        # if the correlation between them is over 0.5 we eliminate the less predictive (for our particular model) column\n",
    "        if (abs(X_train[X_train.columns[i]].corr(X_train[X_train.columns[j]])) > 0.7):\n",
    "            if feature_importances.loc[X_train.columns[i]][0] < feature_importances.loc[X_train.columns[j]][0]:\n",
    "                columns_to_delete.append(feature_importances.loc[X_train.columns[i]].name)\n",
    "            else:\n",
    "                columns_to_delete.append(feature_importances.loc[X_train.columns[j]].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef46246",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_delete.append('Customer_ID')\n",
    "columns_to_delete = list(set(columns_to_delete)) # get unique values\n",
    "columns_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = X_train.drop(columns_to_delete, axis=1)\n",
    "X_val_2 = X_val.drop(columns_to_delete, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e63ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_2 = feature_importances.loc[feature_importances.index.isin(X_train_2)].sort_values(by='Importance', ascending=False)\n",
    "feature_importances_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057baca4",
   "metadata": {},
   "source": [
    "# New model, trained without highly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_2 = RandomForestClassifier(n_estimators=100)\n",
    "rf_model_2.fit(X_train_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a2f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = rf_model_2.predict(X_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred_2, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5067d2b",
   "metadata": {},
   "source": [
    "# XGBoost attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af7bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestClassifier,\n",
    "    StackingClassifier,\n",
    "    HistGradientBoostingClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fb9cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbc_model= GradientBoostingClassifier()\n",
    "gbc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efad3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgboost = gbc_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2dfadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred_xgboost, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe020c59",
   "metadata": {},
   "source": [
    "# Stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(n_jobs=-1)\n",
    "extraTrees = ExtraTreesClassifier(max_depth=10, n_jobs=-1)\n",
    "randomForest = RandomForestClassifier(n_jobs=-1)\n",
    "histGradientBoosting = HistGradientBoostingClassifier()\n",
    "XGB = XGBClassifier(n_jobs=-1)\n",
    "\n",
    "model = StackingClassifier([\n",
    "    ('bagging', bagging),\n",
    "    ('extraTress', extraTrees),\n",
    "    ('randomforest', randomForest),\n",
    "    ('histGradientBoosting', histGradientBoosting),\n",
    "    ('XGB', XGB)\n",
    "], n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c0bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_pred,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4971f",
   "metadata": {},
   "source": [
    "# Attempt to drop irrelevant features with SelectKBest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f55a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "#from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfeatures = SelectKBest(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = bestfeatures.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df799b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureScores = pd.DataFrame(fit.scores_, X_train.columns,  columns=['Importance_Score'])\n",
    "featureScores = featureScores.sort_values(by='Importance_Score', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea81207",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureScores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37451be9",
   "metadata": {},
   "source": [
    "# New model without features with less importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb604af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = X_train[featureScores.index]\n",
    "X_val_3 = X_val[featureScores.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fae647",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val_3)\n",
    "print(classification_report(y_pred,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01334e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in current use\n",
    "import shap\n",
    "\n",
    "def shapley(model, X_train, X_val):\n",
    "    explainer = shap.Explainer(model, X_train)\n",
    "    \n",
    "    shap_values = explainer(X_train)\n",
    "    shap.summary_plot(shap_values, X_train)\n",
    "    # visualize the first prediction's explanation\n",
    "    shap.plots.waterfall(shap_values[0])\n",
    "\n",
    "    # freature importance    \n",
    "    shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "    \n",
    "    shap.plots.bar(shap_values)\n",
    "    shap.summary_plot(shap_values, plot_type='violin')\n",
    "    shap.plots.bar(shap_values[0])\n",
    "    shap.plots.waterfall(shap_values[0])\n",
    "    shap.plots.force(shap_values[0])\n",
    "    \n",
    "    \n",
    "    shap.plots.force(shap_values[1])\n",
    "    \n",
    "    shap.plots.heatmap(shap_values)\n",
    "    \n",
    "    # fig = shap.force_plot(explainer.expected_value, shap_values.values, X_train, feature_names = X_train.columns)\n",
    "    # fig.savefig('testplot.png')\n",
    "    # fig.plot()\n",
    "    \n",
    "    # fig = shap.force_plot(shap_values, X_train)\n",
    "    # fig.plot()\n",
    "   \n",
    "    shap_values = explainer(X_val)\n",
    "    shap.plots.beeswarm(shap_values)\n",
    "    # visualize the first prediction's explanation\n",
    "    shap.plots.waterfall(shap_values[0])\n",
    "    \n",
    "    # freature importance\n",
    "    shap.summary_plot(shap_values, X_val, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
